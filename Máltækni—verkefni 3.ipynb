{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√°lt√¶kni 2022, verkefni 3\n",
    "\n",
    "## T√≥kari\n",
    "\n",
    "### 1. B√∫a til t√≥kara\n",
    "\n",
    "√ç √æessum hluta verkefnisins eigi√∞ √æi√∞ a√∞ b√∫a til ykkar eigin t√≥kara fyrir √≠slensku. Eina forritseiningin sem nota m√° er re √∫r st√∂√∞lu√∞u forritasafni Python (https://docs.python.org/3/library/re.html). √ûi√∞ megi√∞ √æ√≥ nota √∂ll g√∂gn, en ekki hugb√∫na√∞ e√∞a forritas√∂fn, sem √æi√∞ finni√∞ √° netinu.\n",
    "\n",
    "Markmi√∞i√∞ me√∞ t√≥karanum sem √æi√∞ b√∫i√∞ til er a√∞ skipta or√∞um, t√∂lum, greinarmerkjum og skammst√∂funum √≠ einingar. √ûi√∞ √æurfi√∞ ekki a√∞ greina st√¶rri einingar en √æa√∞.\n",
    "\n",
    "D√¶mi: T√≥karafalli√∞ sem √æi√∞ skrifi√∞ √° a√∞ taka setningu s sem inntak og skila t sem √∫ttaki.\n",
    "\n",
    "```python\n",
    "s = 'J√≥n hefur m.a. starfa√∞ sem g√≠tar- og p√≠an√≥kennari, √æ.√° m. hj√° Ice-Music, fr√° √°rinu 1999.'\n",
    "\n",
    "t = ['J√≥n', 'hefur', 'm.a.', 'starfa√∞', 'sem', 'g√≠tar-', 'og', 'p√≠an√≥kennari', ',', '√æ.√° m.', 'hj√°', 'Ice-Music', ',', 'fr√°', '√°rinu', '1999', '.']\n",
    "```\n",
    "\n",
    "Athugi√∞ √æ√≥ a√∞ ekki er n√≥g a√∞ t√≥karinn r√°√∞i vi√∞ fyrstu setninguna heldur √æarf hann a√∞ r√°√∞a vi√∞ a√∞ skipta √∂llum setningum ni√∞ur √≠ or√∞, greinarmerki (passi√∞ bandstrik!), t√∂lur, skammstafanir og √æ√¶r einingar sem ykkur dettur √≠ hug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sj√° √∫tf√¶rslur √° _tveim_ t√≥k√∂rum √≠ [`tokenizer_regex.py`](./tokenizer_regex.py) og [`tokenizer.py`](./tokenizer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Virkni t√≥kara\n",
    "\n",
    "Fjalli√∞ stuttlega um hvernig t√≥karinn ykkar virkar, √æ.e. hva√∞ hann gerir og hvernig. L√Ωsi√∞ √æv√≠ einnig sem hann m√¶tti gera betur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√ç fyrstu langa√∞i mig a√∞ pr√≥fa a√∞fer√∞ sem notar _combinators_ til a√∞ √∫tb√∫a almenna parsera (innbl√°stur fengin fr√° ‚Äû[Building a tiny little broken calculator with parser combinators](https://blog.jfo.click/building-a-tiny-little-broken-calculator-with-parser-combinators/)‚Äú og ‚Äû[Understanding Parser Combinators](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators/)‚Äú).\n",
    "\n",
    "√ûa√∞ √¶vint√Ωri enda√∞i √≠ √∫tf√¶rslu √° [`tokenizer.py`](./tokenizer.py) me√∞ testum √≠ [`tests/tokenizer.py`](./tests/test_tokenizer.py). Allt var √≠ g√≥√∞u, √æar til kom a√∞ √æv√≠ a√∞ keyra √° m√≥ti textum... √û√° kl√°ra√∞ist call-stack hratt og √∂rugglega! Vegna t√≠maleysis f√≥r √©g ekki mj√∂g langt √°√∞ur en √©g √°kva√∞ a√∞ h√¶tta a√∞ reyna a√∞ l√°ta √æa√∞ virka. L√≠klegast er √æetta alls ekki r√©tta t√≥li√∞ fyrir √æetta verkefni√∞ og hentar (bara?) fyrir verkefni √æar sem h√¶gt er a√∞ skilgreina BNF fyrir m√°li√∞.\n",
    "\n",
    "Eftir √æa√∞ var svosem flj√≥tgert a√∞ √∫tf√¶rta regex lausn √≠ [`tokenizer_regex.py`](./tokenizer_regex.py) sem gat √æ√° n√Ωtt sama interface og √æar me√∞ s√∂mu test.\n",
    "\n",
    "√ûa√∞ sem betur m√¶tti fara √≠ regex t√≥kara er:\n",
    "\n",
    "* testa betur, hann keyrir √° m√≥ti testum og textum (eftir a√∞ hafa handb√¶tt vi√∞ t√°knum sem ekki voru studd, t.d. `'¬¥‚Äõ‚Äò‚Ä¶`)\n",
    "* virkar √° m√≥ti √æekktum listum af st√∂fum til einf√∂ldunar √° regex, t.d. er `\\S` ekki nota√∞ til a√∞ matcha √° stafi heldur listi af st√∂fum\n",
    "* takmarka√∞ magn af skammst√∂fum skilgreindar, f√≥r grunnt yfir texta og s√° ekki m√∂rg d√¶mi (enn aftur, t√≠mi) og vildi ekki fl√¶kja √∫tf√¶rslu me√∞ √æv√≠ a√∞ lesa inn lengri lista af skammst√∂funum\n",
    "  * (s√≠√∞an er m√°li√∞ me√∞ a√∞ skilja styttingar VS skammstafanir!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. G√∂gn\n",
    "\n",
    "Hva√∞a g√∂gn, ef einhver, notu√∞u√∞ √æi√∞ og hvernig hj√°lpu√∞u √æau ykkur vi√∞ a√∞ leysa verkefni√∞?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lestur √° blogp√≥stum, stackoverflow sv√∂rum um hvernig regex √≠ Python hagar s√©r og [regex101](https://regex101.com/) til a√∞ pr√≥fa regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. T√≥kun\n",
    "\n",
    "Noti√∞ t√≥karann til √æess a√∞ t√≥ka textana sem √æi√∞ s√∂fnu√∞u√∞ saman √≠ 1. hluta 2. verkefnis. Hverjir eru 10. algengustu t√≥karnir √≠ textanum? Er einhver munur √° √æeim og 10 algengustu or√∞unum √≠ 3.1. √∫r 2. verkefni? Ef textinn var t√≥ka√∞ur skulu√∞ √æi√∞ finna annan texta, a.m.k. 20.000 or√∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_and_p_nouns_from_v2 = [\n",
    "  {'word': 'sister', 'count': 207},\n",
    "  {'word': 'jane', 'count': 196},\n",
    "  {'word': 'time', 'count': 195},\n",
    "  {'word': 'nothing', 'count': 177},\n",
    "  {'word': 'family', 'count': 149},\n",
    "  {'word': 'man', 'count': 146},\n",
    "  {'word': 'day', 'count': 130},\n",
    "  {'word': 'mother', 'count': 124},\n",
    "  {'word': 'letter', 'count': 111},\n",
    "  {'word': 'friend', 'count': 108},\n",
    "  {'word': 'room', 'count': 107},\n",
    "  {'word': 'way', 'count': 102},\n",
    "  {'word': 'manner', 'count': 89},\n",
    "  {'word': 'pleasure', 'count': 89},\n",
    "  {'word': 'anything', 'count': 80},\n",
    "  {'word': '_', 'count': 80},\n",
    "  {'word': 'father', 'count': 80},\n",
    "  {'word': 'aunt', 'count': 79},\n",
    "  {'word': 'hope', 'count': 74},\n",
    "  {'word': 'evening', 'count': 74}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_regex import (\n",
    "    ABBREVIATION,\n",
    "    NUMBER,\n",
    "    PUNCTUATION,\n",
    "    WHITESPACE,\n",
    "    WORD,\n",
    "    tokenize,\n",
    ")\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/pride-and-prejudice.txt\", \"rb\") as input_file:\n",
    "    p_and_p = input_file.read().decode(\"utf8\")\n",
    "    p_and_p_tokens = tokenize(p_and_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'to', 'count': 4090},\n",
       " {'word': 'the', 'count': 4060},\n",
       " {'word': 'of', 'count': 3600},\n",
       " {'word': 'and', 'count': 3427},\n",
       " {'word': 'her', 'count': 2139},\n",
       " {'word': 'I', 'count': 2070},\n",
       " {'word': 'a', 'count': 1897},\n",
       " {'word': 'was', 'count': 1844},\n",
       " {'word': 'in', 'count': 1780},\n",
       " {'word': 'that', 'count': 1527},\n",
       " {'word': 'not', 'count': 1400},\n",
       " {'word': 'she', 'count': 1385},\n",
       " {'word': 'it', 'count': 1288},\n",
       " {'word': 'be', 'count': 1236},\n",
       " {'word': 'his', 'count': 1190},\n",
       " {'word': 'had', 'count': 1152},\n",
       " {'word': 'you', 'count': 1149},\n",
       " {'word': 'as', 'count': 1126},\n",
       " {'word': 'he', 'count': 1101},\n",
       " {'word': 'for', 'count': 1038}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and_p_just_words = [token for token in p_and_p_tokens if token[1] == WORD]\n",
    "\n",
    "p_and_p_counter = Counter([token[0] for token in p_and_p_just_words])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/songlog.txt\", \"rb\") as input_file:\n",
    "    songlog = input_file.read().decode(\"utf8\")\n",
    "    songlog_tokens = tokenize(songlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'og', 'count': 3976},\n",
       " {'word': '√≠', 'count': 2079},\n",
       " {'word': 'er', 'count': 1858},\n",
       " {'word': '√°', 'count': 1693},\n",
       " {'word': 'a√∞', 'count': 1687},\n",
       " {'word': '√©g', 'count': 1479},\n",
       " {'word': 'sem', 'count': 972},\n",
       " {'word': 'vi√∞', 'count': 843},\n",
       " {'word': 'um', 'count': 733},\n",
       " {'word': 'hann', 'count': 661},\n",
       " {'word': 'm√©r', 'count': 590},\n",
       " {'word': '√æa√∞', 'count': 548},\n",
       " {'word': 'me√∞', 'count': 542},\n",
       " {'word': 'var', 'count': 532},\n",
       " {'word': '√æ√∫', 'count': 473},\n",
       " {'word': '√âg', 'count': 468},\n",
       " {'word': 'til', 'count': 439},\n",
       " {'word': 'svo', 'count': 408},\n",
       " {'word': '√æ√°', 'count': 406},\n",
       " {'word': 'h√∫n', 'count': 404}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songlog_words = [token for token in songlog_tokens if token[1] == WORD]\n",
    "\n",
    "p_and_p_counter = Counter([token[0] for token in songlog_words])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athugi√∞ a√∞ √æetta er ekki sanngjarn samanbur√∞ur √æar sem g√∂gn √∫r v2 eru √°n stoppor√∞a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Munur √° t√≥kun og or√∞hlutat√≥kun\n",
    "\n",
    "L√Ωsi√∞ muninum √° hef√∞bundinni t√≥kun og or√∞hlutat√≥kun (sj√° umfj√∂llun √≠ t√≠ma 9. september) √≠ stuttu m√°li. Svari√∞ √æv√≠ hvers vegna vi√∞ notum stundum or√∞hlutat√≥kun fram yfir hef√∞bundna t√≥kun og √∂fugt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hef√∞bundin t√≥kun\n",
    "  * Einfaldari √≠ √∫tf√¶rslu\n",
    "  * S√Ωnir hvernig or√∞ eru notu√∞ √≠ samhengi vi√∞ √∂nnur\n",
    "  * S√Ωnir √∂ll _notu√∞_ or√∞, jafnvel einhver sem vi√∞ √æekkjum ekki\n",
    "  * Heg√∞ar s√©r eins og manneskja myndi vinna verkefni√∞\n",
    "* Or√∞hlutat√≥kun\n",
    "  * Fl√≥knari √≠ √∫tf√¶rslu, krefst √æess a√∞ vi√∞ brj√≥tum or√∞ upp, og hvernig?\n",
    "  * Me√∞ √æv√≠ a√∞ brj√≥ta upp or√∞ getum vi√∞ s√©√∞ (og skili√∞?) or√∞ sem annars v√¶ru √≥√æekkt\n",
    "  * F√°um betri mynd √° st√¶rri gagnasett\n",
    "\n",
    "Notum hef√∞bunda t√≥kun √æegar vi√∞ erum a√∞ byrja a√∞ sko√∞a gagnasett og viljum f√° tilfinningu fyrir √æv√≠.\n",
    "Notum or√∞hlutat√≥kun til a√∞ f√° betri mynd af gagnasetti og √æegar vi√∞ vinnum me√∞ st√¶rri gagnasett til a√∞ f√° raunverulegri tilfinningu fyrir raun innihaldi √æess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lemmur\n",
    "\n",
    "√ç √æessum hluta verkefnisins megi√∞ √æi√∞ nota hva√∞a lemmara sem ykkur dettur √≠ hug, t.d. √∫r √æeim forritas√∂fnum sem fjalla√∞ var um √≠ 2. t√≠ma. Forritasafni√∞ sem s√©r um a√∞ lemma fyrir ykkur m√° einnig sj√° um t√≥kunina √≠ √æessum hluta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lemmur √≠ texta\n",
    "\n",
    "S√¶ki√∞ allar lemmur √∫r textunum sem √æi√∞ s√∂fnu√∞ saman √≠ 1. hluta 2. verkefnis. Er einhver munur √° 10 algengustu lemmunum og 10 algengustu t√≥kunum √≠ 1. 3.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "p_and_p_lemmas = [wnl.lemmatize(word[0]) for word in p_and_p_just_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'to', 'count': 4090},\n",
       " {'word': 'the', 'count': 4060},\n",
       " {'word': 'of', 'count': 3600},\n",
       " {'word': 'and', 'count': 3427},\n",
       " {'word': 'a', 'count': 3023},\n",
       " {'word': 'her', 'count': 2139},\n",
       " {'word': 'I', 'count': 2070},\n",
       " {'word': 'wa', 'count': 1844},\n",
       " {'word': 'in', 'count': 1780},\n",
       " {'word': 'that', 'count': 1527},\n",
       " {'word': 'not', 'count': 1400},\n",
       " {'word': 'it', 'count': 1389},\n",
       " {'word': 'she', 'count': 1385},\n",
       " {'word': 'be', 'count': 1236},\n",
       " {'word': 'his', 'count': 1190},\n",
       " {'word': 'had', 'count': 1152},\n",
       " {'word': 'you', 'count': 1149},\n",
       " {'word': 'he', 'count': 1101},\n",
       " {'word': 'for', 'count': 1038},\n",
       " {'word': 'with', 'count': 1019}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and_p_lemma_counter = Counter([token for token in p_and_p_lemmas])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_lemma_counter.most_common()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einhver or√∞ fara upp og ni√∞ur lista, mestmegnis stoppor√∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmur √°n stoppor√∞a\n",
    "\n",
    "Endurtaki√∞ 1. en hreinsi√∞ n√∫ √∂ll stoppor√∞ √∫r textanum. √ûi√∞ megi√∞ finna stoppor√∞alista √° netinu og nota hann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Mr', 'count': 785},\n",
       " {'word': 's', 'count': 650},\n",
       " {'word': 'Elizabeth', 'count': 634},\n",
       " {'word': 'Darcy', 'count': 417},\n",
       " {'word': 'said', 'count': 402},\n",
       " {'word': 'will', 'count': 402},\n",
       " {'word': 'Mrs', 'count': 344},\n",
       " {'word': 'much', 'count': 324},\n",
       " {'word': 'Bennet', 'count': 323},\n",
       " {'word': 'must', 'count': 307},\n",
       " {'word': 'Bingley', 'count': 306},\n",
       " {'word': 'Jane', 'count': 292},\n",
       " {'word': 'sister', 'count': 292},\n",
       " {'word': 'Miss', 'count': 281},\n",
       " {'word': 'one', 'count': 267},\n",
       " {'word': 'know', 'count': 247},\n",
       " {'word': 'time', 'count': 219},\n",
       " {'word': 'think', 'count': 214},\n",
       " {'word': 'never', 'count': 214},\n",
       " {'word': 'soon', 'count': 213}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/stopwords/en.txt\", \"rb\") as input_file:\n",
    "    en_stop_words = input_file.read().decode(\"utf8\").split()\n",
    "\n",
    "p_and_p_no_stops = [word[0] for word in p_and_p_just_words if word[0].lower() not in en_stop_words]\n",
    "\n",
    "p_and_p_no_stops_lemmas = [wnl.lemmatize(word) for word in p_and_p_no_stops]\n",
    "\n",
    "p_and_p_no_stopa_lemma_counter = Counter([token for token in p_and_p_no_stops_lemmas])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_no_stopa_lemma_counter.most_common()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Notagildi lemma\n",
    "\n",
    "L√Ωsi√∞ notagildi lemma stuttlega. Fjalli√∞ um hven√¶r lemmur eiga vi√∞, frekar en or√∞myndir, og √∂fugt, annars vegar √≠ samhengi vi√∞ greiningu texta, svo sem √≠ leitarv√©lum, og hins vegar √≠ samhengi vi√∞ √æa√∞ √æegar vi√∞ √æj√°lfum gervigreindarl√≠k√∂n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmur √≠ samhengi vi√∞ greiningu texta: n√Ωtum til a√∞ finna ‚Äûgrunn‚Äú or√∞s og √æannig a√∞rar myndir √æess, t.d. til a√∞ geta fundi√∞ ‚Äûmenn‚Äú √æegar leita√∞ a√∞ ‚Äûmennirnir‚Äú og vita√∞ a√∞ vi√∞ h√∂fum samhengi, √≥l√≠kt √æv√≠ a√∞ leita a√∞ ‚Äûmenn*‚Äú og f√° husganleg √≥tengdar ni√∞urst√∂√∞ur eins og ‚Äûmennska‚Äú.\n",
    "\n",
    "Or√∞myndir √≠ samhengi vi√∞ greiningu texta: viljum vita samhengi og beygingar or√∞a √æegar vi√∞ erum a√∞ greina texta t.d. fyrir lei√∞r√©ttingar, hj√°lparforrit sem beygja fyrir okkur.\n",
    "\n",
    "Lemmur √≠ samhengi vi√∞ √æj√°lfun √° gervigreindarl√≠k√∂num: vi√∞ getum fengi√∞ n√°kv√¶mari merkingu √∫r textanum sem vi√∞ erum a√∞ vinna me√∞, en √° kostna√∞ √æess a√∞ vera fl√≥knari a√∞ger√∞ og t√≠mafrekari √≠ t√∂lvuvinnslu, einnig geta veri√∞ or√∞ sem vi√∞ kunnum ekki a√∞ lemma, e√∞a or√∞ sem eru margr√¶√∞ og geta √æannig or√∞i√∞ vitlaus vi√∞ √æj√°lfun (t.d. `minni`).\n",
    "\n",
    "Or√∞myndir √≠ samhengi vi√∞ √æj√°lfun √° gervigreindarl√≠k√∂num: n√Ωtir raun textann og kennir √æv√≠ raunverulegri √∫tg√°fu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-st√¶√∞ur\n",
    "\n",
    "√ç √æessum hluta verkefnisins noti√∞ √æi√∞ textana sem √æi√∞ settu√∞ saman √≠ 1. hluta 2. verkefnis. Noti√∞ t√≥karann sem √æi√∞ bjuggu√∞ til √≠ 1. hluta √æessa verkefnisins til √æess a√∞ skipta textanum √≠ t√≥ka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. √ötb√∫a 3-st√¶√∞ur\n",
    "\n",
    "Skrifi√∞ eigi√∞ fall sem skilar √∂llum 3-st√¶√∞um √∫r textanum. Athugi√∞ a√∞ √æ√¶r eiga a√∞ skarast, √æannig a√∞ ef vi√∞ gefum okkur setninguna\n",
    "\n",
    "```text\n",
    "‚ÄûHundurinn sefur oft √≠ s√≥fanum‚Äú\n",
    "```\n",
    "\n",
    "er √∫ttaki√∞\n",
    "\n",
    "```python\n",
    "[[Hundurinn, sefur, oft], [sefur, oft, √≠], [oft, √≠, s√≥fanum]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram(tokens, n):\n",
    "  ngrams = []\n",
    "\n",
    "  for index in range(len(tokens)):\n",
    "    if index + n <= len(tokens):\n",
    "      ngrams.append(tokens[index : index + n])\n",
    "\n",
    "  return ngrams\n",
    "\n",
    "def ngram_text(text, n):\n",
    "  tokenized = [token[0] for token in tokenize(text) if token[1] == WORD]\n",
    "  return ngram(tokenized, n)\n",
    "\n",
    "ngram_text(\"Hundurinn sefur oft √≠ s√≥fanum\", 3) == [[\"Hundurinn\", \"sefur\", \"oft\"], [\"sefur\", \"oft\", \"√≠\"], [\"oft\", \"√≠\", \"s√≥fanum\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hverjar eru 10 algengustu 3-st√¶√∞urnar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': ('Mr', 'Darcy', 's'), 'count': 29},\n",
       " {'word': ('Mr', 'Bingley', 's'), 'count': 21},\n",
       " {'word': ('Mr', 'Collins', 's'), 'count': 21},\n",
       " {'word': ('Miss', 'de', 'Bourgh'), 'count': 20},\n",
       " {'word': ('Lady', 'Catherine', 's'), 'count': 16},\n",
       " {'word': ('said', 'Mrs', 'Bennet'), 'count': 15},\n",
       " {'word': ('Miss', 'Bingley', 's'), 'count': 15},\n",
       " {'word': ('Lady', 'Catherine', 'de'), 'count': 14},\n",
       " {'word': ('Catherine', 'de', 'Bourgh'), 'count': 14},\n",
       " {'word': ('Mrs', 'Bennet', 's'), 'count': 13},\n",
       " {'word': ('Mr', 'Mrs', 'Gardiner'), 'count': 13},\n",
       " {'word': ('said', 'Mr', 'Bennet'), 'count': 10},\n",
       " {'word': ('Mr', 'Bennet', 's'), 'count': 10},\n",
       " {'word': ('Sir', 'William', 'Lucas'), 'count': 10},\n",
       " {'word': ('said', 'Miss', 'Bingley'), 'count': 10},\n",
       " {'word': ('Mr', 'Wickham', 's'), 'count': 10},\n",
       " {'word': ('Mrs', 'Hurst', 'Miss'), 'count': 9},\n",
       " {'word': ('Hurst', 'Miss', 'Bingley'), 'count': 9},\n",
       " {'word': ('Lady', 'Catherine', 'daughter'), 'count': 8},\n",
       " {'word': ('late', 'Mr', 'Darcy'), 'count': 7}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_grams_p_and_p = ngram(p_and_p_no_stops, 3)\n",
    "three_grams_p_and_p_tuples = [tuple(item) for item in three_grams_p_and_p]\n",
    "\n",
    "three_grams_p_and_p_counter = Counter(three_grams_p_and_p_tuples)\n",
    "[{ 'word': element, 'count': count } for element,count in three_grams_p_and_p_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§¶üèª‚Äç‚ôÇÔ∏è √≠ sta√∞inn fyrir a√∞ fara a√∞ reyna a√∞ laga √æennan b√∂gg √≠ tokenizer, notum NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': (',', '‚Äù', 'said'), 'count': 213},\n",
       " {'word': ('?', '‚Äù', '‚Äú'), 'count': 166},\n",
       " {'word': (',', 'however', ','), 'count': 109},\n",
       " {'word': (',', '‚Äù', 'replied'), 'count': 75},\n",
       " {'word': ('‚Äù', 'said', ','), 'count': 66},\n",
       " {'word': ('Mr.', 'Darcy', ','), 'count': 64},\n",
       " {'word': ('said', ',', '‚Äú'), 'count': 56},\n",
       " {'word': ('Bingley', '‚Äô', 's'), 'count': 49},\n",
       " {'word': (',', '‚Äù', 'cried'), 'count': 47},\n",
       " {'word': ('‚Äú', 'Oh', '!'), 'count': 45},\n",
       " {'word': ('.', 'Mrs.', 'Bennet'), 'count': 43},\n",
       " {'word': ('Darcy', '‚Äô', 's'), 'count': 43},\n",
       " {'word': ('‚Äù', 'said', 'Elizabeth'), 'count': 42},\n",
       " {'word': (',', 'therefore', ','), 'count': 42},\n",
       " {'word': ('‚Äù', '‚Äú', 'Oh'), 'count': 41},\n",
       " {'word': ('Mrs.', 'Bennet', ','), 'count': 41},\n",
       " {'word': ('‚Äù', '‚Äú', ','), 'count': 40},\n",
       " {'word': ('‚Äù', '‚Äú', 'Yes'), 'count': 40},\n",
       " {'word': ('Elizabeth', '‚Äô', 's'), 'count': 38},\n",
       " {'word': ('sister', '‚Äô', 's'), 'count': 38}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = nltk.word_tokenize(p_and_p)\n",
    "tokens_no_stops = [word for word in tokens if word.lower() not in en_stop_words]\n",
    "\n",
    "three_grams_p_and_p_take_two = ngram(tokens_no_stops, 3)\n",
    "three_grams_p_and_p_take_two_tuples = [tuple(item) for item in three_grams_p_and_p_take_two]\n",
    "\n",
    "three_grams_p_and_p_take_two_counter = Counter(three_grams_p_and_p_take_two_tuples)\n",
    "[{ 'word': element, 'count': count } for element,count in three_grams_p_and_p_take_two_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ok, gefst upp √≠ bili"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Notagildi N-st√¶√∞na\n",
    "\n",
    "L√Ωsi√∞ √æv√≠ stuttlega hvert notagildi n-st√¶√∞na er, √æ.e. hven√¶r vi√∞ notum √æ√¶r og til hvers. Fjalli√∞ einnig um hvers vegna forsenda Markovs er ekki fullkomin √æegar kemur a√∞ √æv√≠ a√∞ sp√° fyrir um n√¶sta or√∞ √≠ or√∞arunu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hven√¶r notum vi√∞ N-st√¶√∞ur?\n",
    "N-st√¶√∞ur hafa √Ωmis not √° allskonar svi√∞um. Fyrir m√°lt√¶kni er √æetta nota√∞ til a√∞ greina samhengi √∫t fr√° s√©√∞um texta, stinga upp √° texta, √∫tb√∫a texta sem l√≠kist √∂√∞rum, stinga upp √° or√∞i gefin skrifu√∞ or√∞ o.s.fr\n",
    "\n",
    "Til hvers notum vi√∞ N-st√¶√∞ur?\n",
    "Til √æess a√∞ finna l√≠kur √° a√∞ gefin N-1 or√∞, hvert Nta or√∞i√∞ er. √ûannig f√°um vi√∞  m√≥del sem leyfir okkur me√∞ √°kve√∞inni t√∂lfr√¶√∞ilegri vissu a√∞ gera gera hlutina sem √°√∞ur voru nefndir.\n",
    "\n",
    "Forsenda Markovs er ekki fullkomin √æegar kemur a√∞ √æv√≠ a√∞ sp√° fyrir um n√¶sta or√∞ ef or√∞in sem √° undan koma hafa ekki s√©st √≠ gefnum texta. E√∞a ef textinn hefur marga, jafn l√≠klega m√∂guleika √∫t fr√° or√∞um √≠ st√¶√∞unni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. B√≥nus\n",
    "\n",
    "B√≥nus: Skrifi√∞ fall sem reiknar l√≠kurnar √° √æri√∞ja or√∞i mi√∞a√∞ vi√∞ or√∞in tv√∂ √° undan: P(w3|w1,w2) √∫t fr√° 3-st√¶√∞unum sem √æi√∞ ger√∞u√∞ √≠ fyrsta hluta √æessa hluta verkefnisins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatta ekki alveg √° hva√∞a formi og runninn √∫t √° t√≠ma!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2e14ec575573b8be072e4f3a87b89fe042cbcbb9ac3447dde126ec4009b8f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
