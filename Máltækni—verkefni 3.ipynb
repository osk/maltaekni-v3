{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máltækni 2022, verkefni 3\n",
    "\n",
    "## Tókari\n",
    "\n",
    "### 1. Búa til tókara\n",
    "\n",
    "Í þessum hluta verkefnisins eigið þið að búa til ykkar eigin tókara fyrir íslensku. Eina forritseiningin sem nota má er re úr stöðluðu forritasafni Python (https://docs.python.org/3/library/re.html). Þið megið þó nota öll gögn, en ekki hugbúnað eða forritasöfn, sem þið finnið á netinu.\n",
    "\n",
    "Markmiðið með tókaranum sem þið búið til er að skipta orðum, tölum, greinarmerkjum og skammstöfunum í einingar. Þið þurfið ekki að greina stærri einingar en það.\n",
    "\n",
    "Dæmi: Tókarafallið sem þið skrifið á að taka setningu s sem inntak og skila t sem úttaki.\n",
    "\n",
    "```python\n",
    "s = 'Jón hefur m.a. starfað sem gítar- og píanókennari, þ.á m. hjá Ice-Music, frá árinu 1999.'\n",
    "\n",
    "t = ['Jón', 'hefur', 'm.a.', 'starfað', 'sem', 'gítar-', 'og', 'píanókennari', ',', 'þ.á m.', 'hjá', 'Ice-Music', ',', 'frá', 'árinu', '1999', '.']\n",
    "```\n",
    "\n",
    "Athugið þó að ekki er nóg að tókarinn ráði við fyrstu setninguna heldur þarf hann að ráða við að skipta öllum setningum niður í orð, greinarmerki (passið bandstrik!), tölur, skammstafanir og þær einingar sem ykkur dettur í hug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sjá útfærslur á _tveim_ tókörum í [`tokenizer_regex.py`](./tokenizer_regex.py) og [`tokenizer.py`](./tokenizer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Virkni tókara\n",
    "\n",
    "Fjallið stuttlega um hvernig tókarinn ykkar virkar, þ.e. hvað hann gerir og hvernig. Lýsið því einnig sem hann mætti gera betur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Í fyrstu langaði mig að prófa aðferð sem notar _combinators_ til að útbúa almenna parsera (innblástur fengin frá „[Building a tiny little broken calculator with parser combinators](https://blog.jfo.click/building-a-tiny-little-broken-calculator-with-parser-combinators/)“ og „[Understanding Parser Combinators](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators/)“).\n",
    "\n",
    "Það ævintýri endaði í útfærslu á [`tokenizer.py`](./tokenizer.py) með testum í [`tests/tokenizer.py`](./tests/test_tokenizer.py). Allt var í góðu, þar til kom að því að keyra á móti textum... Þá kláraðist call-stack hratt og örugglega! Vegna tímaleysis fór ég ekki mjög langt áður en ég ákvað að hætta að reyna að láta það virka. Líklegast er þetta alls ekki rétta tólið fyrir þetta verkefnið og hentar (bara?) fyrir verkefni þar sem hægt er að skilgreina BNF fyrir málið.\n",
    "\n",
    "Eftir það var svosem fljótgert að útfærta regex lausn í [`tokenizer_regex.py`](./tokenizer_regex.py) sem gat þá nýtt sama interface og þar með sömu test.\n",
    "\n",
    "Það sem betur mætti fara í regex tókara er:\n",
    "\n",
    "* testa betur, hann keyrir á móti testum og textum (eftir að hafa handbætt við táknum sem ekki voru studd, t.d. `'´‛‘…`)\n",
    "* virkar á móti þekktum listum af stöfum til einföldunar á regex, t.d. er `\\S` ekki notað til að matcha á stafi heldur listi af stöfum\n",
    "* takmarkað magn af skammstöfum skilgreindar, fór grunnt yfir texta og sá ekki mörg dæmi (enn aftur, tími) og vildi ekki flækja útfærslu með því að lesa inn lengri lista af skammstöfunum\n",
    "  * (síðan er málið með að skilja styttingar VS skammstafanir!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gögn\n",
    "\n",
    "Hvaða gögn, ef einhver, notuðuð þið og hvernig hjálpuðu þau ykkur við að leysa verkefnið?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lestur á blogpóstum, stackoverflow svörum um hvernig regex í Python hagar sér og [regex101](https://regex101.com/) til að prófa regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tókun\n",
    "\n",
    "Notið tókarann til þess að tóka textana sem þið söfnuðuð saman í 1. hluta 2. verkefnis. Hverjir eru 10. algengustu tókarnir í textanum? Er einhver munur á þeim og 10 algengustu orðunum í 3.1. úr 2. verkefni? Ef textinn var tókaður skuluð þið finna annan texta, a.m.k. 20.000 orð."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_and_p_nouns_from_v2 = [\n",
    "  {'word': 'sister', 'count': 207},\n",
    "  {'word': 'jane', 'count': 196},\n",
    "  {'word': 'time', 'count': 195},\n",
    "  {'word': 'nothing', 'count': 177},\n",
    "  {'word': 'family', 'count': 149},\n",
    "  {'word': 'man', 'count': 146},\n",
    "  {'word': 'day', 'count': 130},\n",
    "  {'word': 'mother', 'count': 124},\n",
    "  {'word': 'letter', 'count': 111},\n",
    "  {'word': 'friend', 'count': 108},\n",
    "  {'word': 'room', 'count': 107},\n",
    "  {'word': 'way', 'count': 102},\n",
    "  {'word': 'manner', 'count': 89},\n",
    "  {'word': 'pleasure', 'count': 89},\n",
    "  {'word': 'anything', 'count': 80},\n",
    "  {'word': '_', 'count': 80},\n",
    "  {'word': 'father', 'count': 80},\n",
    "  {'word': 'aunt', 'count': 79},\n",
    "  {'word': 'hope', 'count': 74},\n",
    "  {'word': 'evening', 'count': 74}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer_regex import (\n",
    "    ABBREVIATION,\n",
    "    NUMBER,\n",
    "    PUNCTUATION,\n",
    "    WHITESPACE,\n",
    "    WORD,\n",
    "    tokenize,\n",
    ")\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/pride-and-prejudice.txt\", \"rb\") as input_file:\n",
    "    p_and_p = input_file.read().decode(\"utf8\")\n",
    "    p_and_p_tokens = tokenize(p_and_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'to', 'count': 4090},\n",
       " {'word': 'the', 'count': 4060},\n",
       " {'word': 'of', 'count': 3600},\n",
       " {'word': 'and', 'count': 3427},\n",
       " {'word': 'her', 'count': 2139},\n",
       " {'word': 'I', 'count': 2070},\n",
       " {'word': 'a', 'count': 1897},\n",
       " {'word': 'was', 'count': 1844},\n",
       " {'word': 'in', 'count': 1780},\n",
       " {'word': 'that', 'count': 1527},\n",
       " {'word': 'not', 'count': 1400},\n",
       " {'word': 'she', 'count': 1385},\n",
       " {'word': 'it', 'count': 1288},\n",
       " {'word': 'be', 'count': 1236},\n",
       " {'word': 'his', 'count': 1190},\n",
       " {'word': 'had', 'count': 1152},\n",
       " {'word': 'you', 'count': 1149},\n",
       " {'word': 'as', 'count': 1126},\n",
       " {'word': 'he', 'count': 1101},\n",
       " {'word': 'for', 'count': 1038}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and_p_just_words = [token for token in p_and_p_tokens if token[1] == WORD]\n",
    "\n",
    "p_and_p_counter = Counter([token[0] for token in p_and_p_just_words])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/songlog.txt\", \"rb\") as input_file:\n",
    "    songlog = input_file.read().decode(\"utf8\")\n",
    "    songlog_tokens = tokenize(songlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'og', 'count': 3976},\n",
       " {'word': 'í', 'count': 2079},\n",
       " {'word': 'er', 'count': 1858},\n",
       " {'word': 'á', 'count': 1693},\n",
       " {'word': 'að', 'count': 1687},\n",
       " {'word': 'ég', 'count': 1479},\n",
       " {'word': 'sem', 'count': 972},\n",
       " {'word': 'við', 'count': 843},\n",
       " {'word': 'um', 'count': 733},\n",
       " {'word': 'hann', 'count': 661},\n",
       " {'word': 'mér', 'count': 590},\n",
       " {'word': 'það', 'count': 548},\n",
       " {'word': 'með', 'count': 542},\n",
       " {'word': 'var', 'count': 532},\n",
       " {'word': 'þú', 'count': 473},\n",
       " {'word': 'Ég', 'count': 468},\n",
       " {'word': 'til', 'count': 439},\n",
       " {'word': 'svo', 'count': 408},\n",
       " {'word': 'þá', 'count': 406},\n",
       " {'word': 'hún', 'count': 404}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songlog_words = [token for token in songlog_tokens if token[1] == WORD]\n",
    "\n",
    "p_and_p_counter = Counter([token[0] for token in songlog_words])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athugið að þetta er ekki sanngjarn samanburður þar sem gögn úr v2 eru án stopporða."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Munur á tókun og orðhlutatókun\n",
    "\n",
    "Lýsið muninum á hefðbundinni tókun og orðhlutatókun (sjá umfjöllun í tíma 9. september) í stuttu máli. Svarið því hvers vegna við notum stundum orðhlutatókun fram yfir hefðbundna tókun og öfugt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hefðbundin tókun\n",
    "  * Einfaldari í útfærslu\n",
    "  * Sýnir hvernig orð eru notuð í samhengi við önnur\n",
    "  * Sýnir öll _notuð_ orð, jafnvel einhver sem við þekkjum ekki\n",
    "  * Hegðar sér eins og manneskja myndi vinna verkefnið\n",
    "* Orðhlutatókun\n",
    "  * Flóknari í útfærslu, krefst þess að við brjótum orð upp, og hvernig?\n",
    "  * Með því að brjóta upp orð getum við séð (og skilið?) orð sem annars væru óþekkt\n",
    "  * Fáum betri mynd á stærri gagnasett\n",
    "\n",
    "Notum hefðbunda tókun þegar við erum að byrja að skoða gagnasett og viljum fá tilfinningu fyrir því.\n",
    "Notum orðhlutatókun til að fá betri mynd af gagnasetti og þegar við vinnum með stærri gagnasett til að fá raunverulegri tilfinningu fyrir raun innihaldi þess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lemmur\n",
    "\n",
    "Í þessum hluta verkefnisins megið þið nota hvaða lemmara sem ykkur dettur í hug, t.d. úr þeim forritasöfnum sem fjallað var um í 2. tíma. Forritasafnið sem sér um að lemma fyrir ykkur má einnig sjá um tókunina í þessum hluta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lemmur í texta\n",
    "\n",
    "Sækið allar lemmur úr textunum sem þið söfnuð saman í 1. hluta 2. verkefnis. Er einhver munur á 10 algengustu lemmunum og 10 algengustu tókunum í 1. 3.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "p_and_p_lemmas = [wnl.lemmatize(word[0]) for word in p_and_p_just_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'to', 'count': 4090},\n",
       " {'word': 'the', 'count': 4060},\n",
       " {'word': 'of', 'count': 3600},\n",
       " {'word': 'and', 'count': 3427},\n",
       " {'word': 'a', 'count': 3023},\n",
       " {'word': 'her', 'count': 2139},\n",
       " {'word': 'I', 'count': 2070},\n",
       " {'word': 'wa', 'count': 1844},\n",
       " {'word': 'in', 'count': 1780},\n",
       " {'word': 'that', 'count': 1527},\n",
       " {'word': 'not', 'count': 1400},\n",
       " {'word': 'it', 'count': 1389},\n",
       " {'word': 'she', 'count': 1385},\n",
       " {'word': 'be', 'count': 1236},\n",
       " {'word': 'his', 'count': 1190},\n",
       " {'word': 'had', 'count': 1152},\n",
       " {'word': 'you', 'count': 1149},\n",
       " {'word': 'he', 'count': 1101},\n",
       " {'word': 'for', 'count': 1038},\n",
       " {'word': 'with', 'count': 1019}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and_p_lemma_counter = Counter([token for token in p_and_p_lemmas])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_lemma_counter.most_common()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einhver orð fara upp og niður lista, mestmegnis stopporð."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lemmur án stopporða\n",
    "\n",
    "Endurtakið 1. en hreinsið nú öll stopporð úr textanum. Þið megið finna stopporðalista á netinu og nota hann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'Mr', 'count': 785},\n",
       " {'word': 's', 'count': 650},\n",
       " {'word': 'Elizabeth', 'count': 634},\n",
       " {'word': 'Darcy', 'count': 417},\n",
       " {'word': 'said', 'count': 402},\n",
       " {'word': 'will', 'count': 402},\n",
       " {'word': 'Mrs', 'count': 344},\n",
       " {'word': 'much', 'count': 324},\n",
       " {'word': 'Bennet', 'count': 323},\n",
       " {'word': 'must', 'count': 307},\n",
       " {'word': 'Bingley', 'count': 306},\n",
       " {'word': 'Jane', 'count': 292},\n",
       " {'word': 'sister', 'count': 292},\n",
       " {'word': 'Miss', 'count': 281},\n",
       " {'word': 'one', 'count': 267},\n",
       " {'word': 'know', 'count': 247},\n",
       " {'word': 'time', 'count': 219},\n",
       " {'word': 'think', 'count': 214},\n",
       " {'word': 'never', 'count': 214},\n",
       " {'word': 'soon', 'count': 213}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/stopwords/en.txt\", \"rb\") as input_file:\n",
    "    en_stop_words = input_file.read().decode(\"utf8\").split()\n",
    "\n",
    "p_and_p_no_stops = [word[0] for word in p_and_p_just_words if word[0].lower() not in en_stop_words]\n",
    "\n",
    "p_and_p_no_stops_lemmas = [wnl.lemmatize(word) for word in p_and_p_no_stops]\n",
    "\n",
    "p_and_p_no_stopa_lemma_counter = Counter([token for token in p_and_p_no_stops_lemmas])\n",
    "[{ 'word': element, 'count': count } for element,count in p_and_p_no_stopa_lemma_counter.most_common()][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Notagildi lemma\n",
    "\n",
    "Lýsið notagildi lemma stuttlega. Fjallið um hvenær lemmur eiga við, frekar en orðmyndir, og öfugt, annars vegar í samhengi við greiningu texta, svo sem í leitarvélum, og hins vegar í samhengi við það þegar við þjálfum gervigreindarlíkön."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmur í samhengi við greiningu texta: nýtum til að finna „grunn“ orðs og þannig aðrar myndir þess, t.d. til að geta fundið „menn“ þegar leitað að „mennirnir“ og vitað að við höfum samhengi, ólíkt því að leita að „menn*“ og fá husganleg ótengdar niðurstöður eins og „mennska“.\n",
    "\n",
    "Orðmyndir í samhengi við greiningu texta: viljum vita samhengi og beygingar orða þegar við erum að greina texta t.d. fyrir leiðréttingar, hjálparforrit sem beygja fyrir okkur.\n",
    "\n",
    "Lemmur í samhengi við þjálfun á gervigreindarlíkönum: við getum fengið nákvæmari merkingu úr textanum sem við erum að vinna með, en á kostnað þess að vera flóknari aðgerð og tímafrekari í tölvuvinnslu, einnig geta verið orð sem við kunnum ekki að lemma, eða orð sem eru margræð og geta þannig orðið vitlaus við þjálfun (t.d. `minni`).\n",
    "\n",
    "Orðmyndir í samhengi við þjálfun á gervigreindarlíkönum: nýtir raun textann og kennir því raunverulegri útgáfu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-stæður\n",
    "\n",
    "Í þessum hluta verkefnisins notið þið textana sem þið settuð saman í 1. hluta 2. verkefnis. Notið tókarann sem þið bjugguð til í 1. hluta þessa verkefnisins til þess að skipta textanum í tóka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Útbúa 3-stæður\n",
    "\n",
    "Skrifið eigið fall sem skilar öllum 3-stæðum úr textanum. Athugið að þær eiga að skarast, þannig að ef við gefum okkur setninguna\n",
    "\n",
    "```text\n",
    "„Hundurinn sefur oft í sófanum“\n",
    "```\n",
    "\n",
    "er úttakið\n",
    "\n",
    "```python\n",
    "[[Hundurinn, sefur, oft], [sefur, oft, í], [oft, í, sófanum]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngram(tokens, n):\n",
    "  ngrams = []\n",
    "\n",
    "  for index in range(len(tokens)):\n",
    "    if index + n <= len(tokens):\n",
    "      ngrams.append(tokens[index : index + n])\n",
    "\n",
    "  return ngrams\n",
    "\n",
    "def ngram_text(text, n):\n",
    "  tokenized = [token[0] for token in tokenize(text) if token[1] == WORD]\n",
    "  return ngram(tokenized, n)\n",
    "\n",
    "ngram_text(\"Hundurinn sefur oft í sófanum\", 3) == [[\"Hundurinn\", \"sefur\", \"oft\"], [\"sefur\", \"oft\", \"í\"], [\"oft\", \"í\", \"sófanum\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hverjar eru 10 algengustu 3-stæðurnar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': ('Mr', 'Darcy', 's'), 'count': 29},\n",
       " {'word': ('Mr', 'Bingley', 's'), 'count': 21},\n",
       " {'word': ('Mr', 'Collins', 's'), 'count': 21},\n",
       " {'word': ('Miss', 'de', 'Bourgh'), 'count': 20},\n",
       " {'word': ('Lady', 'Catherine', 's'), 'count': 16},\n",
       " {'word': ('said', 'Mrs', 'Bennet'), 'count': 15},\n",
       " {'word': ('Miss', 'Bingley', 's'), 'count': 15},\n",
       " {'word': ('Lady', 'Catherine', 'de'), 'count': 14},\n",
       " {'word': ('Catherine', 'de', 'Bourgh'), 'count': 14},\n",
       " {'word': ('Mrs', 'Bennet', 's'), 'count': 13},\n",
       " {'word': ('Mr', 'Mrs', 'Gardiner'), 'count': 13},\n",
       " {'word': ('said', 'Mr', 'Bennet'), 'count': 10},\n",
       " {'word': ('Mr', 'Bennet', 's'), 'count': 10},\n",
       " {'word': ('Sir', 'William', 'Lucas'), 'count': 10},\n",
       " {'word': ('said', 'Miss', 'Bingley'), 'count': 10},\n",
       " {'word': ('Mr', 'Wickham', 's'), 'count': 10},\n",
       " {'word': ('Mrs', 'Hurst', 'Miss'), 'count': 9},\n",
       " {'word': ('Hurst', 'Miss', 'Bingley'), 'count': 9},\n",
       " {'word': ('Lady', 'Catherine', 'daughter'), 'count': 8},\n",
       " {'word': ('late', 'Mr', 'Darcy'), 'count': 7}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_grams_p_and_p = ngram(p_and_p_no_stops, 3)\n",
    "three_grams_p_and_p_tuples = [tuple(item) for item in three_grams_p_and_p]\n",
    "\n",
    "three_grams_p_and_p_counter = Counter(three_grams_p_and_p_tuples)\n",
    "[{ 'word': element, 'count': count } for element,count in three_grams_p_and_p_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤦🏻‍♂️ í staðinn fyrir að fara að reyna að laga þennan bögg í tokenizer, notum NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': (',', '”', 'said'), 'count': 213},\n",
       " {'word': ('?', '”', '“'), 'count': 166},\n",
       " {'word': (',', 'however', ','), 'count': 109},\n",
       " {'word': (',', '”', 'replied'), 'count': 75},\n",
       " {'word': ('”', 'said', ','), 'count': 66},\n",
       " {'word': ('Mr.', 'Darcy', ','), 'count': 64},\n",
       " {'word': ('said', ',', '“'), 'count': 56},\n",
       " {'word': ('Bingley', '’', 's'), 'count': 49},\n",
       " {'word': (',', '”', 'cried'), 'count': 47},\n",
       " {'word': ('“', 'Oh', '!'), 'count': 45},\n",
       " {'word': ('.', 'Mrs.', 'Bennet'), 'count': 43},\n",
       " {'word': ('Darcy', '’', 's'), 'count': 43},\n",
       " {'word': ('”', 'said', 'Elizabeth'), 'count': 42},\n",
       " {'word': (',', 'therefore', ','), 'count': 42},\n",
       " {'word': ('”', '“', 'Oh'), 'count': 41},\n",
       " {'word': ('Mrs.', 'Bennet', ','), 'count': 41},\n",
       " {'word': ('”', '“', ','), 'count': 40},\n",
       " {'word': ('”', '“', 'Yes'), 'count': 40},\n",
       " {'word': ('Elizabeth', '’', 's'), 'count': 38},\n",
       " {'word': ('sister', '’', 's'), 'count': 38}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = nltk.word_tokenize(p_and_p)\n",
    "tokens_no_stops = [word for word in tokens if word.lower() not in en_stop_words]\n",
    "\n",
    "three_grams_p_and_p_take_two = ngram(tokens_no_stops, 3)\n",
    "three_grams_p_and_p_take_two_tuples = [tuple(item) for item in three_grams_p_and_p_take_two]\n",
    "\n",
    "three_grams_p_and_p_take_two_counter = Counter(three_grams_p_and_p_take_two_tuples)\n",
    "[{ 'word': element, 'count': count } for element,count in three_grams_p_and_p_take_two_counter.most_common()][:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ok, gefst upp í bili"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Notagildi N-stæðna\n",
    "\n",
    "Lýsið því stuttlega hvert notagildi n-stæðna er, þ.e. hvenær við notum þær og til hvers. Fjallið einnig um hvers vegna forsenda Markovs er ekki fullkomin þegar kemur að því að spá fyrir um næsta orð í orðarunu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvenær notum við N-stæður?\n",
    "N-stæður hafa ýmis not á allskonar sviðum. Fyrir máltækni er þetta notað til að greina samhengi út frá séðum texta, stinga upp á texta, útbúa texta sem líkist öðrum, stinga upp á orði gefin skrifuð orð o.s.fr\n",
    "\n",
    "Til hvers notum við N-stæður?\n",
    "Til þess að finna líkur á að gefin N-1 orð, hvert Nta orðið er. Þannig fáum við  módel sem leyfir okkur með ákveðinni tölfræðilegri vissu að gera gera hlutina sem áður voru nefndir.\n",
    "\n",
    "Forsenda Markovs er ekki fullkomin þegar kemur að því að spá fyrir um næsta orð ef orðin sem á undan koma hafa ekki sést í gefnum texta. Eða ef textinn hefur marga, jafn líklega möguleika út frá orðum í stæðunni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bónus\n",
    "\n",
    "Bónus: Skrifið fall sem reiknar líkurnar á þriðja orði miðað við orðin tvö á undan: P(w3|w1,w2) út frá 3-stæðunum sem þið gerðuð í fyrsta hluta þessa hluta verkefnisins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatta ekki alveg á hvaða formi og runninn út á tíma!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae2e14ec575573b8be072e4f3a87b89fe042cbcbb9ac3447dde126ec4009b8f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
